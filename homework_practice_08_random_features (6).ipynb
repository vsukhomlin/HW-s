{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "homework-practice-08-random-features.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTHVx2OCKFFU",
        "colab_type": "text"
      },
      "source": [
        "# Машинное обучение, ФКН ВШЭ\n",
        "\n",
        "## Практическое задание 8. Метод опорных векторов и аппроксимация ядер\n",
        "\n",
        "### Общая информация\n",
        "Дата выдачи: 15.03.2020\n",
        "\n",
        "Мягкий дедлайн: 02:59MSK 30.03.2020\n",
        "\n",
        "Жесткий дедлайн: 23:59MSK 03.04.2020\n",
        "\n",
        "### Оценивание и штрафы\n",
        "Каждая из задач имеет определенную «стоимость» (указана в скобках около задачи). Максимальная оценка за работу (без учёта бонусов) — 10 баллов.\n",
        "\n",
        "Сдавать задание после указанного жёсткого срока сдачи нельзя. При выставлении неполного балла за задание в связи с наличием ошибок на усмотрение проверяющего предусмотрена возможность исправить работу на указанных в ответном письме условиях.\n",
        "\n",
        "Задание выполняется самостоятельно. «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов (подробнее о плагиате см. на странице курса). Если вы нашли решение какого-то из заданий (или его часть) в открытом источнике, необходимо указать ссылку на этот источник в отдельном блоке в конце вашей работы (скорее всего вы будете не единственным, кто это нашел, поэтому чтобы исключить подозрение в плагиате, необходима ссылка на источник).\n",
        "\n",
        "Неэффективная реализация кода может негативно отразиться на оценке.\n",
        "\n",
        "### Формат сдачи\n",
        "Задания сдаются через систему anytask. Посылка должна содержать:\n",
        "* Ноутбук homework-practice-08-random-features-Username.ipynb\n",
        "\n",
        "Username — ваша фамилия и имя на латинице именно в таком порядке"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9GyNAtsKFFX",
        "colab_type": "text"
      },
      "source": [
        "### О задании\n",
        "\n",
        "На занятиях мы подробно обсуждали метод опорных векторов (SVM). В базовой версии в нём нет чего-то особенного — мы всего лишь используем специальную функцию потерь, которая не требует устремлять отступы к бесконечности; ей достаточно, чтобы отступы были не меньше +1. Затем мы узнали, что SVM можно переписать в двойственном виде, который, позволяет заменить скалярные произведения объектов на ядра. Это будет соответствовать построению модели в новом пространстве более высокой размерности, координаты которого представляют собой нелинейные модификации исходных признаков.\n",
        "\n",
        "Ядровой SVM, к сожалению, довольно затратен по памяти (нужно хранить матрицу Грама размера $d \\times d$) и по времени (нужно решать задачу условной оптимизации с квадратичной функцией, а это не очень быстро). Мы обсуждали, что есть способы посчитать новые признаки $\\tilde \\varphi(x)$ на основе исходных так, что скалярные произведения этих новых $\\langle \\tilde \\varphi(x), \\tilde \\varphi(z) \\rangle$ приближают ядро $K(x, z)$.\n",
        "\n",
        "Мы будем исследовать аппроксимации методом Random Kitchen Sinks для гауссовых ядер. Будем использовать формулы, которые немного отличаются от того, что было на лекциях (мы добавим сдвиги внутрь тригонометрических функций и будем использовать только косинусы, потому что с нужным сдвигом косинус превратится в синус):\n",
        "$$\\tilde \\varphi(x) = (\n",
        "\\cos (w_1^T x + b_1),\n",
        "\\dots,\n",
        "\\cos (w_n^T x + b_n)\n",
        "),$$\n",
        "где $w_j \\sim \\mathcal{N}(0, 1/\\sigma^2)$, $b_j \\sim U[-\\pi, \\pi]$.\n",
        "\n",
        "На новых признаках $\\tilde \\varphi(x)$ мы будем строить любую линейную модель.\n",
        "\n",
        "Можно считать, что это некоторая новая парадигма построения сложных моделей. Можно направленно искать сложные нелинейные закономерности в данных с помощью градиентного бустинга или нейронных сетей, а можно просто нагенерировать большое количество случайных нелинейных признаков и надеяться, что быстрая и простая модель (то есть линейная) сможет показать на них хорошее качество. В этом задании мы изучим, насколько работоспособна такая идея.\n",
        "\n",
        "### Алгоритм\n",
        "\n",
        "Вам потребуется реализовать следующий алгоритм:\n",
        "1. Понизить размерность выборки до new_dim с помощью метода главных компонент.\n",
        "2. Для полученной выборки оценить гиперпараметр $\\sigma^2$ с помощью эвристики (рекомендуем считать медиану не по всем парам объектов, а по случайному подмножеству из где-то миллиона пар объектов): $$\\sigma^2 = \\text{median}_{i, j = 1, \\dots, \\ell, i \\neq j} \\left\\{\\sum_{k = 1}^{d} (x_{ik} - x_{jk})^2 \\right\\}$$\n",
        "3. Сгенерировать n_features наборов весов $w_j$ и сдвигов $b_j$.\n",
        "4. Сформировать n_features новых признаков по формулам, приведённым выше.\n",
        "5. Обучить линейную модель (логистическую регрессию или SVM) на новых признаках.\n",
        "6. Повторить преобразования (PCA, формирование новых признаков) к тестовой выборке и применить модель."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryxhkdy7KFFY",
        "colab_type": "text"
      },
      "source": [
        "Тестировать алгоритм мы будем на данных Fashion MNIST. Ниже код для их загрузки и подготовки."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyyBESzLKFFZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "887e5f82-d747-46c4-b144-83db4b0ab2ee"
      },
      "source": [
        "import keras\n",
        "from keras.datasets import fashion_mnist\n",
        "(x_train_pics, y_train), (x_test_pics, y_test) = fashion_mnist.load_data()\n",
        "x_train = x_train_pics.reshape(x_train_pics.shape[0],-1)\n",
        "x_test = x_test_pics.reshape(x_test_pics.shape[0],-1)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 3us/step\n",
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 2s 0us/step\n",
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cYfcTzmKZt1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.decomposition import PCA\n",
        "from statistics import median\n",
        "from math import sqrt\n",
        "from math import pi\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "We6y2VF6KFFd",
        "colab_type": "text"
      },
      "source": [
        "__Задание 1. (5 баллов)__\n",
        "\n",
        "Реализуйте алгоритм, описанный выше. Желательно в виде класса с методами fit() и predict().\n",
        "\n",
        "Что должно быть в вашей реализации:\n",
        "1. Возможность задавать значения гиперпараметров new_dim (по умолчанию 50) и n_features (по умолчанию 1000).\n",
        "2. Возможность включать или выключать предварительное понижение размерности с помощью метода главных компонент.\n",
        "3. Возможность выбирать тип линейной модели (логистическая регрессия или SVM с линейным ядром).\n",
        "\n",
        "Протестируйте на данных Fashion MNIST, сформированных кодом выше. Если на тесте у вас получилась доля верных ответов не ниже 0.84 с гиперпараметрами по умолчанию, то вы всё сделали правильно."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZKgFfyUKFFe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.base import BaseEstimator\n",
        "\n",
        "\n",
        "class Taskone():\n",
        "\n",
        "    def __init__(self, new_dim = 50, n_features = 1000, pca = True, svm = False, kernel='linear', regularisation=1, tolerance=1e-3, max_iter=10000):\n",
        "\n",
        "        self.new_dim = new_dim\n",
        "        self.n_features = n_features\n",
        "        self.pca = pca\n",
        "        self.svm = svm\n",
        "        self.model = None \n",
        "        self.pcamodel = None\n",
        "        self.B = []\n",
        "        self.W = []\n",
        "        self.kernel = kernel\n",
        "        self.regularisation = regularisation\n",
        "        self.tolerance = tolerance\n",
        "        self.max_iter = max_iter\n",
        "\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "\n",
        "        #понижение размерности\n",
        "        if self.pca == True:\n",
        "            self.pcamodel = PCA(n_components=self.new_dim)\n",
        "            self.pcamodel.fit(X)\n",
        "            X = self.pcamodel.transform(X)\n",
        "\n",
        "        #подсчет сигмы\n",
        "        pares = []\n",
        "        limit = min(1000, X.shape[0])   #если наблюдений меньше 1000, то итерируем в пределах всей выборки\n",
        "        sigma = 0\n",
        "        dim2 = X.shape[1]\n",
        "        for i in range(limit):\n",
        "            for j in range (limit):\n",
        "                sum = 0\n",
        "                if i != j:\n",
        "                    for k in range(dim2):\n",
        "                        sum += (X[i][k] - X[j][k])**2\n",
        "                    pares.append(sum)\n",
        "                else:\n",
        "                    pass\n",
        "        sigma = median(pares)\n",
        "\n",
        "        #создание новых признаков\n",
        "        SE = sqrt(1/sigma)\n",
        "        dim1 = X.shape[0]        \n",
        "        self.B = np.random.uniform(low=-pi, high=pi, size=(1, self.n_features))[0].transpose()\n",
        "        Xnew = np.zeros(shape=(X.shape[0], self.n_features)).tolist()\n",
        "        for j in range(self.n_features):\n",
        "            W = np.random.normal(loc=0, scale=SE, size=(1, dim2))[0]\n",
        "            self.W.append(W)  \n",
        "            for i in range(dim1):\n",
        "                Xnew[i][j] = X[i].dot(W) + self.B[j]\n",
        "        X = Xnew                                           \n",
        "        \n",
        "        #обучаем модель на полученных данных\n",
        "        if self.svm == False:\n",
        "            self.model = LogisticRegression(C=self.regularisation, tol=self.tolerance, max_iter=self.max_iter)\n",
        "            self.model.fit(X = X, y = y)\n",
        "        else:\n",
        "            self.model = SVC(kernel=self.kernel, C=self.regularisation, tol=self.tolerance)\n",
        "            self.model.fit(X = X, y = y)\n",
        "\n",
        "        return self\n",
        "\n",
        "        \n",
        "    def predict(self, X):\n",
        "\n",
        "        #адаптировать выборку\n",
        "        if self.pca == True:\n",
        "            X = self.pcamodel.transform(X)\n",
        "        dim1 = X.shape[0] \n",
        "        Xnew = np.zeros(shape=(X.shape[0], self.n_features)).tolist()\n",
        "        for j in range(self.n_features):\n",
        "            for i in range(dim1):\n",
        "                Xnew[i][j] = X[i].dot(self.W[j]) + self.B[j]\n",
        "        X = Xnew                \n",
        "\n",
        "        #применить модель\n",
        "        if self.svm == False:\n",
        "            y_pred = self.model.predict(X)\n",
        "        else:\n",
        "            y_pred = self.model.predict(X)\n",
        "\n",
        "        return y_pred\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1IR2j9sKjqg",
        "colab_type": "code",
        "outputId": "3eaaafdc-a016-4b1b-da29-92a47de40d3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "Model = Taskone(svm=True)\n",
        "Model.fit(X = x_train, y=y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.Taskone at 0x7efc1ecefc18>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3guKxb5ex9XH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = Model.predict(X = x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "am9gc_O0MWVi",
        "colab_type": "code",
        "outputId": "b196c46b-3a7e-44da-af93-f377924728c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "accuracy_score(y_test, y_pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8355"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tK00-F1HOYwk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#как видим всё классно работает, ура"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSOasVxRKFFj",
        "colab_type": "text"
      },
      "source": [
        "__Задание 2. (3 балла)__\n",
        "\n",
        "Сравните подход со случайными признаками с обучением SVM на исходных признаках. Попробуйте вариант с обычным (линейным) SVM и с ядровым SVM. Ядровой SVM может очень долго обучаться, поэтому можно делать любые разумные вещи для ускорения: брать подмножество объектов из обучающей выборки, например.\n",
        "\n",
        "Сравните подход со случайными признаками с вариантом, в котором вы понижаете размерность с помощью PCA и обучаете градиентный бустинг (используйте нормальную имплементацию, а не из sklearn, и подберите число деревьев и длину шага).\n",
        "\n",
        "Сделайте выводы — насколько идея со случайными признаками работает? Сравните как с точки зрения качества, так и с точки зрения скорости обучения и применения."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGRG0HeyKFFk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train_mini = x_train[0:5000][:]\n",
        "y_train_mini = y_train[0:5000][:]\n",
        "#вообще по-хорошему надо было взять хотя бы 15000-20000 наблюдений, но в своем юпитере у меня не получается открыть данные - что-то ему не нравится, а колаб постоянно \n",
        "#находит повод отключиться, пока модель обучается 25 раз"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOPw4Ds4vX21",
        "colab_type": "code",
        "outputId": "c48508bd-ba28-4218-a237-a98a7dfe4510",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "#SVM линейный на исходных:\n",
        "#будем перебирать два основных гиперпараметра: C (соответствует линии в аутпуте - чем больше, тем ниже), tol (соответствует столбику в аутпуте - чем меньше, тем правее)\n",
        "res = np.zeros(shape=(5,5)).tolist()\n",
        "line = 0\n",
        "vertic = 0\n",
        "for i in ([1, 1.5, 2, 2.5, 3]):\n",
        "    for j in ([1e-1, 1e-3, 1e-5, 1e-7, 1e-9]):\n",
        "        SVMlinear = SVC(kernel='linear', C=i, tol=j)\n",
        "        SVMlinear.fit(x_train_mini, y_train_mini)\n",
        "        y_pred = SVMlinear.predict(X = x_test)\n",
        "        res[line][vertic] = accuracy_score(y_test, y_pred)\n",
        "        vertic += 1\n",
        "    line += 1\n",
        "    vertic = 0\n",
        "    print(line)\n",
        "res"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.7978, 0.7976, 0.7976, 0.7976, 0.7976],\n",
              " [0.7978, 0.7976, 0.7976, 0.7976, 0.7976],\n",
              " [0.7978, 0.7976, 0.7976, 0.7976, 0.7976],\n",
              " [0.7978, 0.7976, 0.7976, 0.7976, 0.7976],\n",
              " [0.7978, 0.7976, 0.7976, 0.7976, 0.7976]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLl9rh06EQgx",
        "colab_type": "code",
        "outputId": "b6caa5ea-d5a5-40f5-95cd-eb426e2d951f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "#SVM ядровой (гауссовское) на исходных:\n",
        "#будем перебирать два основных гиперпараметра: C, tol\n",
        "res = np.zeros(shape=(5,5)).tolist()\n",
        "line = 0\n",
        "vertic = 0\n",
        "for i in ([1, 1.5, 2, 2.5, 3]):\n",
        "    for j in ([1e-1, 1e-3, 1e-5, 1e-7, 1e-9]):\n",
        "        SVMnonlin = SVC(C=i, tol=j)\n",
        "        SVMnonlin.fit(x_train_mini, y_train_mini)\n",
        "        y_pred = SVMnonlin.predict(X = x_test)\n",
        "        res[line][vertic] = accuracy_score(y_test, y_pred)\n",
        "        vertic += 1\n",
        "    line += 1\n",
        "    vertic = 0\n",
        "    print(line)\n",
        "res"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.8347, 0.8343, 0.8343, 0.8343, 0.8343],\n",
              " [0.8426, 0.8427, 0.8427, 0.8427, 0.8427],\n",
              " [0.8478, 0.8469, 0.8469, 0.8469, 0.8469],\n",
              " [0.8487, 0.8489, 0.8489, 0.8489, 0.8489],\n",
              " [0.8502, 0.8503, 0.8504, 0.8504, 0.8504]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nz-QIsaUGp0A",
        "colab_type": "code",
        "outputId": "f862c9e9-1d11-4ca8-d4ae-77f0e0f371f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "#наша модель, SVM ядровой (гауссовское):\n",
        "#будем перебирать два основных гиперпараметра: C, tol\n",
        "res = np.zeros(shape=(5,5)).tolist()\n",
        "line = 0\n",
        "vertic = 0\n",
        "for i in ([1, 1.5, 2, 2.5, 3]):\n",
        "    for j in ([1e-1, 1e-3, 1e-5, 1e-7, 1e-9]):\n",
        "        SVMmodelnonlin = Taskone(svm=True, kernel='rbf', regularisation=i, tolerance=j)\n",
        "        SVMmodelnonlin.fit(x_train_mini, y_train_mini)\n",
        "        y_pred = SVMmodelnonlin.predict(X = x_test)\n",
        "        res[line][vertic] = accuracy_score(y_test, y_pred)\n",
        "        vertic += 1\n",
        "    line += 1\n",
        "    vertic = 0\n",
        "    print(line)\n",
        "res"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.7928, 0.7917, 0.7947, 0.7938, 0.7922],\n",
              " [0.8006, 0.803, 0.8024, 0.8027, 0.8022],\n",
              " [0.8083, 0.8066, 0.8066, 0.8083, 0.8068],\n",
              " [0.8116, 0.8106, 0.8106, 0.8125, 0.8134],\n",
              " [0.8139, 0.8133, 0.8143, 0.8157, 0.8146]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjps9rFeM6sJ",
        "colab_type": "code",
        "outputId": "4507659c-6a75-4890-aad4-b16a4fbb3c57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "#наша модель, SVM линейный:\n",
        "#будем перебирать два основных гиперпараметра: C, tol\n",
        "res = np.zeros(shape=(5,5)).tolist()\n",
        "line = 0\n",
        "vertic = 0\n",
        "for i in ([1, 1.5, 2, 2.5, 3]):\n",
        "    for j in ([1e-1, 1e-3, 1e-5, 1e-7, 1e-9]):\n",
        "        SVMmodellinear = Taskone(svm=True, regularisation=i, tolerance=j)\n",
        "        SVMmodellinear.fit(x_train_mini, y_train_mini)\n",
        "        y_pred = SVMmodellinear.predict(X = x_test)\n",
        "        res[line][vertic] = accuracy_score(y_test, y_pred)\n",
        "        vertic += 1\n",
        "    line += 1\n",
        "    vertic = 0\n",
        "    print(line)\n",
        "res"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.8102, 0.8105, 0.808, 0.811, 0.8087],\n",
              " [0.8066, 0.8083, 0.81, 0.8077, 0.8088],\n",
              " [0.8087, 0.8084, 0.8083, 0.8082, 0.8081],\n",
              " [0.8083, 0.8079, 0.8073, 0.8065, 0.8105],\n",
              " [0.8075, 0.8073, 0.8088, 0.8068, 0.8088]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJ-zD2BCxNkr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import xgboost as xgb\n",
        "pca = PCA(n_components=50)\n",
        "pca.fit(x_train_mini)\n",
        "x_train_forxgb = pca.transform(x_train_mini)\n",
        "dtrain = xgb.DMatrix(x_train_forxgb, label=y_train_mini)\n",
        "x_test_forxgb = pca.transform(x_test)\n",
        "dtest = xgb.DMatrix(x_test_forxgb, label=y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ow4gbQYU1q09",
        "colab_type": "code",
        "outputId": "8ecdd2e2-14f3-4b4a-963f-e85786aef711",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "#как было сказано в задании, перебираем длинну шага (eta, соответсвует строкам в аутпуте - чем ниже, тем больше) и кол-во деревьев (соотв. столбикам, чем их больше, тем правее)\n",
        "res = np.zeros(shape=(5,5)).tolist()\n",
        "line = 0\n",
        "vertic = 0\n",
        "for i in ([0.1, 0.3, 0.5, 0.7, 1]):\n",
        "    params = {'eta': i, 'objective': 'multi:softmax', 'num_class': 10}\n",
        "    for j in ([50, 70, 90, 110, 130]):\n",
        "        model_boosting = xgb.train(params=params, dtrain = dtrain, num_boost_round=j)\n",
        "        y_pred = model_boosting.predict(dtest)\n",
        "        res[line][vertic] = accuracy_score(y_test, y_pred)\n",
        "        vertic += 1\n",
        "    line += 1\n",
        "    vertic = 0\n",
        "    print(line)\n",
        "res"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.8098, 0.8135, 0.8167, 0.8192, 0.8211],\n",
              " [0.8216, 0.8223, 0.8249, 0.8257, 0.8264],\n",
              " [0.8173, 0.8178, 0.8192, 0.8199, 0.8197],\n",
              " [0.8154, 0.8158, 0.8167, 0.8166, 0.8178],\n",
              " [0.8053, 0.8055, 0.8063, 0.8076, 0.8068]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kP86xA3lTgr_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ну в общем возможно на бОльших данных наша модель (как и бустинг) проявила бы себя лучше чем SVM, но как я написал выше, не представляется такое возможным.\n",
        "#по результатам тестов лучший результат показал ядровой SVM, за ним - бустинг, затем - наша модель с ядровым SVM\n",
        "#я забыл подрубать считалку времени и вспомнил о ней только после того, как уже всё прогнал :) но как я отчетливо помню, линейный SVM и бустинг обучались на порядок быстрее;\n",
        "#ядровой SVM обучался сопоставимо долго с нашей моделью\n",
        "#короче говоря если выбирать между нашей моделью и альтернативами, то бустинг во всём выиграл. *с учетом того, что данных мало взять получилось\n",
        "#а еще я забыл указать рандом сиды, надеюсь это не отразится глобально на перфомансе моделей :)\n",
        "#а еще про существование такой штуки как гридсерч я узнал вечером 29ого читая телегу, так что не судите строго"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SBqOFW8KFFn",
        "colab_type": "text"
      },
      "source": [
        "__Задание 3. (2 балла)__\n",
        "\n",
        "Проведите эксперименты:\n",
        "1. Помогает ли предварительное понижение размерности с помощью PCA? \n",
        "2. Как зависит итоговое качество от n_features? Выходит ли оно на плато при росте n_features?\n",
        "3. Важно ли, какую модель обучать — логистическую регрессию или SVM?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ndyUQOOKFFo",
        "colab_type": "code",
        "outputId": "d3ebb558-4deb-4cde-d0e9-63359bd84c6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#1ое задание\n",
        "Model = Taskone(pca=False, svm=True, kernel='rbf', regularisation=3, tolerance=1e-7)\n",
        "Model.fit(X = x_train_mini.astype(int), y=y_train_mini)\n",
        "y_pred = Model.predict(X = x_test)\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8176"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyhZ-XbiidvG",
        "colab_type": "code",
        "outputId": "41379dbe-d7ca-416e-d68e-3e29d017c179",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#аналогичная модель (без МГК) была протестирована ранее выше перебором гиперпараметров и дала практически такой же результат на данных настройках\n",
        "#возможно, понижение размерности не помогает из-за того, что объем данных невелик. тогда сравним с первоначальным тестом, на полной выборке, но без МГК:\n",
        "Model = Taskone(pca=False, svm=True)\n",
        "Model.fit(X = x_train.astype(int), y=y_train)\n",
        "y_pred = Model.predict(X = x_test)\n",
        "accuracy_score(y_test, y_pred)\n",
        "\n",
        "#как видим и тут качество практически не изменилось (меняется кстати в лучшую сторону оба раза)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8411"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiXx0ogT89uk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#в общем я либо неправильно проверил (по хорошему надо было ведь так же наборы гиперпараметров смотреть, а не сравнить пару тестов с двумя старыми),\n",
        "#либо понижение размерности при помощи PCA приводит лишь к ускорению алгоритма (обучение на полной выборке за пол часа с понижением против часа без) без изменения качества"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmVVbDby7GIK",
        "colab_type": "code",
        "outputId": "b525ac35-179b-4657-a46b-876491c3b18f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "#2ое задание:\n",
        "x_train_mini = x_train[0:10000][:]\n",
        "y_train_mini = y_train[0:10000][:]\n",
        "\n",
        "res = []\n",
        "for i in ([500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000]):\n",
        "    Model = Taskone(svm=True, kernel='rbf', n_features=i, regularisation=3, tolerance=1e-7)\n",
        "    Model.fit(X = x_train_mini, y=y_train_mini)\n",
        "    y_pred = Model.predict(X = x_test)\n",
        "    res.append(accuracy_score(y_test, y_pred))\n",
        "    print(i)\n",
        "res"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "500\n",
            "1000\n",
            "1500\n",
            "2000\n",
            "2500\n",
            "3000\n",
            "3500\n",
            "4000\n",
            "4500\n",
            "5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.8293, 0.8297, 0.8297, 0.829, 0.8304, 0.8286, 0.829, 0.8307, 0.8305, 0.8305]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tm9iv1qHQ50N",
        "colab_type": "code",
        "outputId": "4cd51796-7c75-4419-a15c-428648c100d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "res = []\n",
        "for i in ([100, 150, 200, 250, 300, 350, 400, 450]):\n",
        "    Model = Taskone(svm=True, kernel='rbf', n_features=i, regularisation=3, tolerance=1e-7)\n",
        "    Model.fit(X = x_train_mini, y=y_train_mini)\n",
        "    y_pred = Model.predict(X = x_test)\n",
        "    res.append(accuracy_score(y_test, y_pred))\n",
        "    print(i)\n",
        "res"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100\n",
            "150\n",
            "200\n",
            "250\n",
            "300\n",
            "350\n",
            "400\n",
            "450\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.8236, 0.8261, 0.826, 0.8253, 0.8298, 0.8279, 0.829, 0.8291]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhJtV4fOPOOj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#с учетом того, что наши фичи генерируются случайным образом, можно сказать что, независимо от от их количества, качество модели остается неизменным\n",
        "#в целом да, с ростом количества фичей показатель качества начинает сходиться (в моем случае это было примерно 0.8305, но как писал раньше, я везде забыл зафиксировать рэндом сид),\n",
        "#хотя и едва заметно - от малого числа до 5000 он возрос всего на 0.01 (даже немного меньше)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aH2P9mr-QZfh",
        "colab_type": "code",
        "outputId": "919ba104-5cce-4ac7-90da-8309cf69c98c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "#3ее задание\n",
        "\n",
        "#мы уже перебирали гиперпараметры для нашей модели для случая SVM. как показала практика, МГК приводит к значительному ускорению обучения и неизменному качеству;\n",
        "#поэтому будем перебирать два основных гиперпараметра (как делали с SVM): C, tol\n",
        "#используя при этом разложение МГК. Сравнивать будем с результатами ядрового SVM, так как у него было результаты лучше.\n",
        "res = np.zeros(shape=(5,5)).tolist()\n",
        "line = 0\n",
        "vertic = 0\n",
        "for i in ([1, 1.5, 2, 2.5, 3]):\n",
        "    for j in ([1e-1, 1e-3, 1e-5, 1e-7, 1e-9]):\n",
        "        SVMmodellog = Taskone(regularisation=i, tolerance=j)\n",
        "        SVMmodellog.fit(x_train_mini.astype(int), y_train_mini)\n",
        "        y_pred = SVMmodellog.predict(X = x_test)\n",
        "        res[line][vertic] = accuracy_score(y_test, y_pred)\n",
        "        vertic += 1\n",
        "    line += 1\n",
        "    vertic = 0\n",
        "    print(line)\n",
        "res"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.8203, 0.8211, 0.8201, 0.8211, 0.8205],\n",
              " [0.8206, 0.8194, 0.8204, 0.8211, 0.8206],\n",
              " [0.8206, 0.821, 0.8215, 0.8212, 0.8202],\n",
              " [0.8217, 0.8211, 0.8203, 0.8205, 0.8217],\n",
              " [0.8213, 0.8205, 0.8203, 0.8213, 0.8199]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5p0AvD6v9WOo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#так как данных было использовано в два раза больше, качество незначительно выше, но в целом модель с SVM отличается от модели с Лог регрессией только тем, \n",
        "#что имеет одинаковое значение качества независимо от гиперпараметров - с учетом того, что обучалась она так же долго, я бы отдал предпочтение ей, как более стабильной"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dyrE788KFFr",
        "colab_type": "text"
      },
      "source": [
        "### Бонус\n",
        "\n",
        "Ниже приведены задания на бонусные баллы. За исследования на стандартных датасетах из sklearn или, скажем, на титанике, баллы не будут начисляться. Приветствуются интересные выводы из проведённых экспериментов."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxt9SxxRKFFs",
        "colab_type": "text"
      },
      "source": [
        "__Задание 4. (Максимум 2 балла)__\n",
        "\n",
        "Возьмите какой-нибудь достаточно сложный датасет, на котором хорошо работает градиентный бустинг. Сравните бустинг с нашим алгоритмом со случайными признаками с точки зрения качества и скорости. Подберите как следует гиперпараметры алгоритма."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFTkEu1qKFFt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6E_xWzWAKFFx",
        "colab_type": "text"
      },
      "source": [
        "__Задание 5. (Максимум 2 балла)__\n",
        "\n",
        "Возьмите какой-нибудь датасет с текстами и решите одним из стандартных методов (например, tf-idf + логистическая регрессия или что-то нейросетевое). Сравните по качеству и скорости с нашим алгоритмом."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neuisP4ZKFFy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_XzMk8NKFF1",
        "colab_type": "text"
      },
      "source": [
        "__Задание 6. (Максимум 2 балла)__\n",
        "\n",
        "Поэкспериментируйте на нескольких датасетах из прошлых пунктов с фукнциями для вычисления новых случайных признаков. Не обязательно использовать косинус от скалярного произведения — можно брать знак от него, хэш и т.д. Придумайте побольше вариантов для генерации признаков и проверьте, не получается ли с их помощью добиваться более высокого качества."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpKBrIFwKFF2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}